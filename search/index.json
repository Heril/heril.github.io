[{"content":"  To Be Finished\nfor (i in 1:length(luckfactor)) { for (j in 1:length(selected)) { for (k in 1:R) { simulationresults \u0026lt;- tibble(Skill = runif(n = N, min = minscore, max = maxscore), Luck = runif(n = N, min = minscore, max = maxscore)) %\u0026gt;% mutate(Score = Skill * (1 - luckfactor[i]) + Luck * luckfactor[i]) luckyFew \u0026lt;- simulationresults %\u0026gt;% mutate(SkillRank = N - rank(Skill) + 1, ScoreRank = N - rank(Score) + 1, SkillSelected = SkillRank \u0026lt;= selected[j] * N) %\u0026gt;% arrange(ScoreRank) %\u0026gt;% head(selected[j] * N) results[[i]][[j]][k,] \u0026lt;- c(mean(luckyFew$Skill), mean(luckyFew$Luck), sum(luckyFew$SkillSelected), luckfactor[i], selected[j]) } results[[i]][[j]] \u0026lt;- as.data.frame(results[[i]][[j]]) } results[[i]] \u0026lt;- bind_rows(results[[i]]) } results \u0026lt;- bind_rows(results) colnames(results) \u0026lt;- c(\u0026quot;Skill\u0026quot;, \u0026quot;Luck\u0026quot;, \u0026quot;SkillSelected\u0026quot;, \u0026quot;LuckFactor\u0026quot;, \u0026quot;ProportionSelected\u0026quot;) results$LuckFactor \u0026lt;- as.factor(results$LuckFactor) results$ProportionSelectedfct \u0026lt;- as.factor(results$ProportionSelected) results %\u0026gt;% group_by(LuckFactor, ProportionSelectedfct) %\u0026gt;% summarize(meanSkill = mean(Skill), meanLuck = mean(Luck), meanSelected = mean(SkillSelected), medianSkill = median(Skill), medianLuck = median(Luck), medianSelected = median(SkillSelected), propSelected = mean(SkillSelected/ProportionSelected)/N) ## `summarise()` has grouped output by \u0026#39;LuckFactor\u0026#39;. You can override using the `.groups` argument. ## # A tibble: 16 x 9 ## # Groups: LuckFactor [4] ## LuckFactor ProportionSelectedfct meanSkill meanLuck meanSelected medianSkill ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0.01 5e-04 99.9 89.5 2.2 99.9 ## 2 0.01 0.001 99.8 85.9 8.2 99.8 ## 3 0.01 0.01 99.5 57.6 176. 99.5 ## 4 0.01 0.1 95.0 51.4 1975. 95.0 ## 5 0.02 5e-04 99.8 92.4 2.3 99.9 ## 6 0.02 0.001 99.8 89.5 6.1 99.8 ## 7 0.02 0.01 99.3 66.4 154. 99.3 ## 8 0.02 0.1 95.1 52.1 1950. 95.1 ## 9 0.05 5e-04 99.7 95.9 1.1 99.8 ## 10 0.05 0.001 99.6 93.3 3.5 99.6 ## 11 0.05 0.01 98.9 79.7 104. 98.9 ## 12 0.05 0.1 94.9 54.7 1872. 94.9 ## 13 0.1 5e-04 99.7 96.7 1 99.7 ## 14 0.1 0.001 99.5 95.7 2.4 99.5 ## 15 0.1 0.01 98.4 86.3 78.9 98.5 ## 16 0.1 0.1 94.5 59.5 1728. 94.5 ## # … with 3 more variables: medianLuck \u0026lt;dbl\u0026gt;, medianSelected \u0026lt;dbl\u0026gt;, ## # propSelected \u0026lt;dbl\u0026gt; ","date":"2021-06-27","permalink":"http://blog.elementaldatascience.com/2021/06/27/simmulating-the-effects-of-luck-in-highly-competivie-events/","tags":["Data Science","R","Simulation"],"title":"Simmulating the effects of luck in highly competivie events"},{"content":"Earlier this year, I posted the first part of a write-up for a project I worked on during a project I worked on last year.\nAs a reminder, my project involved taking 4 different types of correlation, test the baseline and 8 different data shifts, generating 1000 simulations with 100 points of data each. I then compared how three different multivariate control charts, Hotelling T^2, MEWMA, and MCUSUM, were in detecting a shift while holding the false rate fixed over all simulations.\nFor this I use the pretty standard for in control average run length (ARL) of 300. This means, on average when the process is in control, you can expect to see an out of control data point in any 300 sequential points. The simple way to do this is to calculate the control limits for each control chart using the in-control data so that this constraint is kept. For this code, the variable cARL is set to 300 to represent constraining the ARL to be 300 in this case.\nt2ucl \u0026lt;- simData %\u0026gt;% filter(shift == \u0026quot;0/0\u0026quot;) %\u0026gt;% group_by(copula, rho) %\u0026gt;% summarize(UCLt2 = quantile(t2, probs = (cARL - 1)/cARL)) %\u0026gt;% select(copula, rho, UCLt2) %\u0026gt;% spread(rho, UCLt2)%\u0026gt;% column_to_rownames(var=\u0026quot;copula\u0026quot;) mewmaucl \u0026lt;- simData %\u0026gt;% filter(shift == \u0026quot;0/0\u0026quot;) %\u0026gt;% group_by(copula, rho) %\u0026gt;% summarize(UCLme = quantile(mewma, probs = (cARL - 1)/cARL)) %\u0026gt;% select(copula, rho, UCLme) %\u0026gt;% spread(rho, UCLme)%\u0026gt;% column_to_rownames(var=\u0026quot;copula\u0026quot;) mcusumucl \u0026lt;- simData %\u0026gt;% filter(shift == \u0026quot;0/0\u0026quot;) %\u0026gt;% group_by(copula, rho) %\u0026gt;% summarize(UCLmc = quantile(mcusum, probs = (cARL - 1)/cARL)) %\u0026gt;% select(copula, rho, UCLmc) %\u0026gt;% spread(rho, UCLmc)%\u0026gt;% column_to_rownames(var=\u0026quot;copula\u0026quot;)  From these control limits, we can test the out of control data, and determine the ARL, where the lower the value, the quicker a shift is detected. For ease of use, if a run has zero points out of control, I will treat that as an ARL equal to the size of the run, in each of these cases that being 1000.\ntmpData \u0026lt;- list() index \u0026lt;- 1 for(i in 1:Ncopula) { for(j in 1:Nrho) { tmpData[[index]] \u0026lt;- splitData[[index]] %\u0026gt;% group_by(copula, rho, shift, iteration) %\u0026gt;% summarize(t2ARL = ifelse(shift == \u0026quot;0/0\u0026quot;, size/sum(t2 \u0026gt; t2ucl[i,j]), detect_index(t2, function(z)(z\u0026gt;t2ucl[i,j]))), meARL = ifelse(shift == \u0026quot;0/0\u0026quot;, size/sum(mewma \u0026gt; mewmaucl[i,j]), detect_index(mewma, function(z)(z\u0026gt;mewmaucl[i,j]))), mcARL = ifelse(shift == \u0026quot;0/0\u0026quot;, size/sum(mcusum \u0026gt; mcusumucl[i,j]), detect_index(mcusum, function(z)(z\u0026gt;mcusumucl[i,j])))) %\u0026gt;% mutate(t2ARL = ifelse(is.infinite(t2ARL), size, t2ARL), meARL = ifelse(is.infinite(meARL), size, meARL), mcARL = ifelse(is.infinite(mcARL), size, mcARL)) index \u0026lt;- index + 1 } } rm(splitData)  All that remains is to get those ARL values in a way that is easy to visualize, as well as generate confidence intervals for the simulation as a whole.\nARL \u0026lt;- bind_rows(tmpData) %\u0026gt;% group_by(copula, rho, shift) %\u0026gt;% summarize(t2ci = list(mean_cl_normal(t2ARL) %\u0026gt;% rename(t2ARLmean=y, t2ARLlwr=ymin, t2ARLupr=ymax)), meci = list(mean_cl_normal(meARL) %\u0026gt;% rename(meARLmean=y, meARLlwr=ymin, meARLupr=ymax)), mcci = list(mean_cl_normal(mcARL) %\u0026gt;% rename(mcARLmean=y, mcARLlwr=ymin, mcARLupr=ymax))) %\u0026gt;% unnest(cols = c(t2ci, meci, mcci))  Now that we have our control chart ARL metrics, let\u0026rsquo;s check that the in-control ARL is approximately 300. This corresponds when the column shift is 0/0, meaning the mean for both variables is 0. I\u0026rsquo;m selecting columns to exclude the upper and lower values for the confidence intervals.\n# A tibble: 16 x 6 # Groups: copula, rho [16] copula rho shift t2ARLmean meARLmean mcARLmean \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; 1 Normal 0.6 0/0 300. 299. 299. 2 Normal 0.2 0/0 300. 300. 299. 3 Normal -0.2 0/0 300. 300. 299. 4 Normal -0.6 0/0 300. 299. 301. 5 Frank 0.6 0/0 300. 299. 300. 6 Frank 0.2 0/0 300. 299. 300. 7 Frank -0.2 0/0 300. 300. 298. 8 Frank -0.6 0/0 299. 299. 301. 9 Clayton 0.6 0/0 300. 300. 300. 10 Clayton 0.2 0/0 300. 299. 299. 11 Clayton -0.2 0/0 299. 300. 299. 12 Clayton -0.6 0/0 299. 300. 299. 13 Gumbel 0.6 0/0 300. 301. 299. 14 Gumbel 0.2 0/0 300. 299. 300. 15 Gumbel -0.2 0/0 299. 299. 301. 16 Gumbel -0.6 0/0 299. 300. 299.  Things are looking good, with everything being right around 300. We\u0026rsquo;ve got a lot of plots we can make to visualize this, but for illustrating this simply, let\u0026rsquo;s first look at the simplest, the Hotelling T^2 control chart, looking at a sample control chart as well as the ARL values for both in control and out of control for the different correlation.\nLet\u0026rsquo;s take a look at two of the runs for the Normal copula with a correlation of 0.6, one for when there is no shift, and one where just one of the two variables has shifted by 3.\nWe see in this case with no shift, that only 3 points fall above the upper control limit (UCL), which for 1000 points, that is what we would roughly expect to encounter.\nWhen simulating one of the variables being out of control with a shift of 3, we see we get a lot of points above the UCL and registered as out of control. This is exactly what we would like to happen, as in a production environment, we would want to detect this change and take action to correct it.\nFinally, let\u0026rsquo;s look at an overall summary of the performance for Hotelling T^2 charts for these different copulas and correlations.\nThe curious thing we see with these four graphs is that how well the T^2 performs for detecting different shifts varies between copula type and the strength of the correlation. For my next and final post on this project, I want to delve into the different types of correlations we\u0026rsquo;re working with here and how they are structured and how that changes the performance of the Hotelling T^2 control chart.\n","date":"2021-05-26","permalink":"http://blog.elementaldatascience.com/2021/05/26/multivariate-statistic-process-control-method-comparison-part-2-generating-control-limits-and-initial-look/","tags":["R","SPC","Process Control","Simulation","Trend Analysis"],"title":"Multivariate Statistic Process Control method comparison Part 2: Generating   control limits and initial look "},{"content":"  Probabilities and games.\nTwo simple games used to illustrate certain features of calculating probabilities are as follows:\nYou and a friend take turns flipping a fair coin. The first person to flip heads wins. If you go first, what are your chances of winning?\nYou and a friend take turns rolling a fair die with 6 sides. The first person to roll a 6 wins. If you go first, what are your chances of winning.\nThese end up being very similar problems, and can be generalized.\nFor convenience, I’ll talk about the rolling a die version. To start we’ll walk through a couple winning scenarios to look for a pattern.\nThe simplest way to win when you go first is for you roll a 6 on your first turn. On a fair die, this is simply 1/6.\nIf you don’t win on your first turn, what’s the next simplest way to win? Your first roll to not be a 6, your partner’s first roll to also not be a 6, and then your second roll to be a 6. The odds of each of these independent events is respectively 5/6, 5/6, and 1/6. The odds of this happening in sequence is the product of these,\n\\[ \\frac{5}{6} \\times \\frac{5}{6} \\times \\frac{1}{6} = 11.6\\%\\]\nIt might be clear at this point, but every scenario where when you go first that you win, you end with rolling a 6 (1/6 odds) and you and your opponent having an equal number of turns that no one rolled a 6 (5/6 each for both you and your partner), where this can happen anywhere from 0 or more times.\nWhat then are the odds that you win when you go first on your second turn or sooner? Simply the sum of your odds of winning on the first turn, and the odds of winning on your second turn.\n\\[\\frac{1}{6} + \\frac{25}{216} = \\frac{61}{216} = 28.2\\%\\]\nSo to find out your absolute odds of winning when you go first you just need to keep on adding up these winning scenarios. Using infinite sums you can calculate this\n\\[\\sum_{n=1}^{\\infty}\\left(\\frac{1}{6}\\right)\\left(\\frac{5}{6}\\right)^{2n-2}\\]\nWhere n is the number of turns you take.\nFor those familiar with infinite sums of geometric series, this can be more conveniently written as\n\\[\\sum_{n=1}^{\\infty}\\left(\\frac{1}{6}\\right)\\left(\\frac{25}{36}\\right)^{n-1}\\]\n(Because 5/6 squared is 25/36)\nThis fits the form of the common form of when 0\u0026lt;r\u0026lt;1\n\\[\\sum_{n=1}^{\\infty}ar^{n-1} = \\frac{a}{1-r}\\] So the odds of this is \\[\\frac{\\frac{1}{6}}{1 - \\frac{25}{36}} = \\frac{\\frac{1}{6}}{\\frac{11}{36}} = \\frac{6}{11} = 54.5454\\%\\]\nGiven the common form above, this can be generalized for every probability game of the same nature such as flipping coins or other games where you and a partner take turns and the odds of the current player winning is constant.\nIf the odds of winning during any of your turns is p, where 0\u0026lt;p\u0026lt;1, then the odds of winning at some point is\n\\[\\frac{p}{1-(1-p)^2}\\]\nSimplifying this gives\n\\[\\frac{p}{1-(1-2p+p^2)} = \\frac{p}{2p - p^2} = \\frac{1}{2-p}\\]\nTaking a look at any possible value for p between 0 and 1, we can see that in going first you always have at least a 50% chance of winning, with those odds increasing as p approaches 1.\ngraph\n ","date":"2021-04-23","permalink":"http://blog.elementaldatascience.com/2021/04/23/odds-of-winning-constant-probability-games-against-one-person/","tags":["Probability"],"title":"Odds of winning constant probability games against one person"},{"content":"Two weeks ago I wrote a post, Using R and Random Forest to predict Diabetes Risk. Since I am less experienced with using python in machine learning models, and this was a data set that worked out so nicely, I figured I would take an attempt at it.\nFirst we need to load all the modules and functions we need to use.\nimport pandas as pd from matplotlib import pyplot as plt import numpy as np import seaborn as sns from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn import metrics  The next thing is just like was done in R, load the data, clean it up a bit for using scikit-learn to create a classification model, and then split our factors from our classification variable.\ndiabetesdf = pd.read_csv(\u0026quot;https://archive.ics.uci.edu/ml/machine-learning-databases/00529/diabetes_data_upload.csv\u0026quot;) gender_state = {'Male': 1, 'Female': 0} yes_no_map = {'Yes': 1, 'No': 0} diabetesdf['Gender'] = diabetesdf['Gender'].map(gender_state) for i in range(2,16): diabetesdf.iloc[:,i] = diabetesdf.iloc[:,i].map(yes_no_map) Y = diabetesdf['class'].values X = diabetesdf.drop(labels = ['class'], axis =1)  As we did in R, we need to do a train/test split. In this case, I will set it to be the same split as what I did in R, so that we can compare the accuracy.\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state = 1)  Finally, it\u0026rsquo;s time to make the model and quantify the accuracy. I will set n_estimators = 500, as that matches the default ntrees of randomForest in R.\nmodel = RandomForestClassifier(n_estimators = 500, random_state = 2) model.fit(X_train, Y_train) prediction_test = model.predict(X_test) print(\u0026quot;Accuracy = \u0026quot;, metrics.accuracy_score(Y_test, prediction_test))  Accuracy = 0.9807692307692307 We see here an accuracy of 98.1%. This is a bit better than what we got doing R (97.1%), but not very much different. In fact, when I run the model in R repeatedly I frequently get results of 96.1%, 97.1% and 98.1%, or given the train/test split, this means between 100–102 of the 104 in the test set are accurately predicted. So it looks like we are getting comparable accuracy.\nNow, last time I visualized an overall tree, representing the total random forest. This time, let\u0026rsquo;s do a different sort of visualization. I took a function I found on analyseup on how to make a bar plot to show a bar plot of how important the features are in order.\nfeature_importance = np.array(model.feature_importances_) feature_names = np.array(X.columns) #Create a DataFrame using a Dictionary data={'feature_names':feature_names,'feature_importance':feature_importance} fi_df = pd.DataFrame(data) #Sort the DataFrame in order decreasing feature importance fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True) #Define size of bar plot plt.figure(figsize=(10,8)) #Plot Searborn bar chart sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names']) plt.title('RANDOM FOREST ' + 'FEATURE IMPORTANCE') plt.xlabel('FEATURE IMPORTANCE') plt.ylabel('FEATURE NAMES')  Looking at this, it appears Polyuria (excess urination), Polydipsia (excess thirst or drinking), Age, and Gender are the most important features in predicting Diabetes risk, with other features following afterwards.\nThis was a fun exercise for me, and I look forward to leveraging python more in my future posts and personal machine learning work.\n","date":"2021-01-08","permalink":"http://blog.elementaldatascience.com/2021/01/08/using-python-and-random-forest-to-predict-diabetes-risk/","tags":["Python","Data Science","Machine Learning","Random Forest"],"title":"Using Python and Random Forest to predict Diabetes Risk."},{"content":"Time to mix it up a bit from my usual game probability related simulation. This week I want to talk about a project I worked on last year in doing simulations for Statistical Process Control.\nFor a brief primer, Statistical Process Control or SPC is a method used for monitoring something like a manufacturing process. It\u0026rsquo;s a form of time series analysis where you monitor results of some statistic taken from observing the process and then compare that statistic to a set of limits on a control chart to determine whether or not the distribution of that statistic has likely changed. Ideally, by the time you see a data point is off you can investigate it and fix it before you actually harm product.\nI was initially introduced to the idea of SPC with my internship and proceeded to discuss with a professor about conducting a project on the subject. The project was looking at what type of control chart or statistic did the best job for different kinds of dependent data. In the case of SPC, a better job is for a fixed sensitivity in falsely detecting a change, how quickly can you detect a change of differing magnitudes.\nTo carry out this in a simulation required generating quite a bit of data and then calculating the appropriate control chart statistics from this data. I was going to do four different dependence structures with 4 different correlation strengths each and for each of these I was going to do a baseline simulation and 8 different mean shifts of the data, for a total of 144 different sets of simulations. For each of these I as going to do 1000 simulations of 1000 pairs of data each, with control chart statistics calculated for 3 different control charts, Hotelling T^2, MEWMA, and MCUSUM.\nTo generate the dependent pairs of data, I leveraged the copula package from R and used the MSQC package to calculate the statistics. When I started running this I found that with how much data I was trying to produce and crunch that this was going to take nearly 24 hours of compute time. So, I decided to bring in some help by using the R parallellization packages foreach, doMC and doRNG, as well as rewrote the functions of the MSQC package to cut out the plotting unnecessary plotting to speed things up.\nIn the end I got something a lot more concise and readable, but also could compute within 3 hours, which given that I was using 4 cores meant more time savings than just from using more of my CPU.\nYou can see the code on my CopulaControl repository, or in summary below. Here for brevity I just use MSQC, but at github I have my modified functions from MSQC instead.\nlibrary(copula) library(MSQC) library(tidyverse) library(Hmisc) library(foreach) library(doMC) library(doRNG) copulaMvdGen \u0026lt;- function(myCopula, shift = c(0,0)) { return(mvdc(copula = myCopula, margins = c(\u0026quot;norm\u0026quot;, \u0026quot;norm\u0026quot;), paramMargins = list(list(mean = shift[1], sd = 1), list(mean = shift[2], sd = 1)))) } mvdGen \u0026lt;- function (category = normalCopula, rho = 0.6, shiftList = c(0.5, 1, 2, 3)) { myCopula \u0026lt;- category(iRho(category(), rho)) myMvd \u0026lt;- list() myMvd[[1]] \u0026lt;- copulaMvdGen(myCopula) for(i in 1:length(shiftList)) { myMvd[[1 + i]] \u0026lt;- copulaMvdGen(myCopula, c(0,shiftList[i])) myMvd[[1 + i + length(shiftList)]] \u0026lt;- copulaMvdGen(myCopula, rep(shiftList[i], 2)) } return(myMvd) } # Set number of workers, i.e. number of threads to give to R. # Number of Iterations in generator.R is split up by this. registerDoMC(4) seed \u0026lt;- registerDoRNG(20200218) # Test parameters. # copulaList: Bivariate Copula functions to use to generate simulation data # Change not supported in run.R, need to change in mvdGen() call in generator.R # rhoList: Different correlations to be forced in the bivariate copulas # Any value in interval (-1,1) is allowed. # shiftList: List of shift in means of the bivariate data. Listed is for single # and double variable shifts. Change is not supported in run.R, need to change # mvdGen() call in generator.R to change. # size: Number of pairs of points to generate for each iteration of each experiment. # iterations: Number of repitions for each experiment. # cARL: ARL to calibrate UCL for analysis copulaList \u0026lt;- c(\u0026quot;Normal\u0026quot;, \u0026quot;Frank\u0026quot;, \u0026quot;Clayton\u0026quot;, \u0026quot;Gumbel\u0026quot;) rhoList \u0026lt;- c(0.6, 0.2, -0.2, -0.6) shiftList \u0026lt;- c(\u0026quot;0/0\u0026quot;, \u0026quot;0/0.5\u0026quot;, \u0026quot;0/1\u0026quot;, \u0026quot;0/2\u0026quot;, \u0026quot;0/3\u0026quot;, \u0026quot;0.5/0.5\u0026quot;, \u0026quot;1/1\u0026quot;, \u0026quot;2/2\u0026quot;, \u0026quot;3/3\u0026quot;) size \u0026lt;- 1000 iterations \u0026lt;- 1000 cARL \u0026lt;- 300 Nrho \u0026lt;- length(rhoList) Ncopula \u0026lt;- length(copulaList) Nshift \u0026lt;- length(shiftList)  I was then able to generate my data using a nested for loop. If I were rewriting this less than six months from when I last touched this, I would use something like expand.grid to make a list of my experiments and then iterate the master experiment list instead of so many loops.\nsimData \u0026lt;- vector(\u0026quot;list\u0026quot;, Nrho) for(i in 1:Nrho) { myMvd \u0026lt;- list(mvdGen(normalCopula, rho = rhoList[i]), mvdGen(frankCopula, rho = rhoList[i]), mvdGen(claytonCopula, rho = rhoList[i]), mvdGen(gumbelCopula, rho = rhoList[i])) simData[[i]] \u0026lt;- vector(\u0026quot;list\u0026quot;, Ncopula) for(j in 1:Ncopula) { simData[[i]][[j]] \u0026lt;- foreach(k=1:iterations, .packages=\u0026quot;copula\u0026quot;) %dorng% { tmpData \u0026lt;- vector(\u0026quot;list\u0026quot;, Nshift) for(l in 1:Nshift) { tmpVar \u0026lt;- as.data.frame(rMvdc(size, myMvd[[j]][[l]])) if (l == 1) { t2chart \u0026lt;- mult.chart(tmpVar, type = \u0026quot;t2\u0026quot;) mechart \u0026lt;- mult.chart(tmpVar, type = \u0026quot;mewma\u0026quot;) mcchart \u0026lt;- mult.chart(tmpVar, type = \u0026quot;mcusum\u0026quot;) t2data \u0026lt;- t2chart$t2 medata \u0026lt;- mechart$t2 mcdata \u0026lt;- mcchart$t2 } else { t2data \u0026lt;- mult.chart(tmpVar, type = \u0026quot;t2\u0026quot;, Xmv = t2chart$Xmv, S = t2chart$covariance)$t2 medata \u0026lt;- mult.chart(tmpVar, type = \u0026quot;mewma\u0026quot;, Xmv = mechart$Xmv, S = mechart$covariance)$t2 mcdata \u0026lt;- mult.chart(tmpVar, type = \u0026quot;mcusum\u0026quot;, Xmv = mcchart$Xmv, S = mcchart$covariance)$t2 } tmpData[[l]] \u0026lt;- cbind(rep(copulaList[[j]]), rep(rhoList[[i]]), rep(shiftList[[l]]), rep(k, size), tmpVar, 1:size, t2data, medata, mcdata ) colnames(tmpData[[l]]) \u0026lt;- c(\u0026quot;copula\u0026quot;, \u0026quot;rho\u0026quot;, \u0026quot;shift\u0026quot;, \u0026quot;iteration\u0026quot;, \u0026quot;data1\u0026quot;, \u0026quot;data2\u0026quot;, \u0026quot;N\u0026quot;, \u0026quot;t2\u0026quot;, \u0026quot;mewma\u0026quot;, \u0026quot;mcusum\u0026quot;) } tmpData } } } gc() tmpData \u0026lt;- vector(\u0026quot;list\u0026quot;, Nrho) for(i in 1:Nrho) { tmpData[[i]] \u0026lt;- vector(\u0026quot;list\u0026quot;, Ncopula) for(j in 1:Ncopula) { tmpData[[i]][[j]] \u0026lt;- bind_rows(simData[[i]][[j]]) } } rm(simData) gc() simData \u0026lt;- bind_rows(tmpData) rm(tmpData) gc() simData$copula \u0026lt;- factor(simData$copula, level = copulaList) simData$rho \u0026lt;- factor(simData$rho, level = rhoList) simData$shift \u0026lt;- factor(simData$shift, level = shiftList)  Even with the loops, it becomes much easier to follow than my earlier iterations, where here I loop over the correlations, the copulas, the 1000 iterations, the shifts and generate the data for each of these before bringing it all together.\nI will look at this again to talk in further detail about how I proceeded to generate the visualizations and came to my conclusions from this simulation in part 2. For now I\u0026rsquo;ll just leave one of the plots I made for this.\n","date":"2021-01-04","permalink":"http://blog.elementaldatascience.com/2021/01/04/multivariate-statistic-process-control-method-comparison-part-1-simulating-correlated-data-and-optimizing-data-generation/","tags":["R","Simulation","Process Control","SPC","Multivariate"],"title":"Multivariate Statistic Process Control method comparison Part 1: Simulating   correlated data and optimizing data generation."},{"content":"  So, after my last Dungeons and Dragons post, I got asked about the Advantage/Disadvantage mechanic in Dungeons and Dragons Fifth Edition, so let’s look into that a bit.\nIn D\u0026amp;D, to succeed at a task, you roll a 20-sided die(d20) and with some math and checking with your Dungeon Master (DM), you determine if you succeed or not. In previous editions you would add your character’s modifier, as well as any additional bonuses or penalties due to the specific scenario. Fifth edition got rid of that except for one specific case, and instead replaced it with Advantage/Disadvantage.\nSimply put, if your character is in some sort of advantageous situation when attempting a task, you get Advantage. In a disadvantageous situation? Disadvantage. If you were in a combination of scenarios providing at least one advantage and one disadvantage then you just attempted normally. For a normal situation you roll your d20, then add your character’s relevant ability modifier and proficiency bonus if applicable. For Advantage, you roll your die twice, taking the higher result before adding in your modifiers. Disadvantage is the same, except you take the lower result.\nThis simplifies and streamlines things a lot for the DM, and instead of needing to memorize a bunch of rules and their modifiers, you just need to memorize those rules. From there it’s easy to reason out if you get Advantage or Disadvantage.\nOf course, this has gotten many people wondering how this compares to a modifier to the roll. The answer is not so straightforward, as you’ll see that it depends upon the target result you need to succeed. First, let’s populate a tibble with Results, their Probabilities, and the Probability for beating or exceeding a result. This will include regular, Advantage, and Disadvantage.\nlibrary(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ── ## ✓ ggplot2 3.3.3 ✓ purrr 0.3.4 ## ✓ tibble 3.1.2 ✓ dplyr 1.0.6 ## ✓ tidyr 1.1.3 ✓ stringr 1.4.0 ## ✓ readr 1.4.0 ✓ forcats 0.5.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() library(ggthemes) advdisdf \u0026lt;- expand.grid(list(1:20,1:20)) maxvals \u0026lt;- apply(advdisdf, 1, function(x) max(x)) minvals \u0026lt;- apply(advdisdf, 1, function(x) min(x)) advdistdf \u0026lt;- tibble(R1 = advdisdf$Var1, R2 = advdisdf$Var2, Advantage = maxvals, Disadvantage = minvals) AdvP \u0026lt;- summary(as.factor(advdistdf$Advantage))/400 DisP \u0026lt;- summary(as.factor(advdistdf$Disadvantage))/400 ACDF \u0026lt;- rev(cumsum(rev(AdvP))) DCDF \u0026lt;- rev(cumsum(rev(DisP))) RegP \u0026lt;- summary(as.factor(1:20))/20 CDF \u0026lt;- seq(1,0.05,by = -0.05) ADPMF \u0026lt;- tibble(Result = 1:20, AdvP, ACDF, DisP, DCDF, RegP, CDF) Let’s plot what looks a bit like a “leaf”, showing the increase in probability of beating a result with Advantage or beating a result with Disadvantage.\nADPMF %\u0026gt;% ggplot() + geom_line(aes(x = Result, y = ACDF), size = 2) + geom_line(aes(x = Result, y = DCDF), size = 2) + geom_ribbon(aes(x = Result, ymin = CDF, ymax = ACDF), fill = \u0026quot;green4\u0026quot;, alpha = 0.5) + geom_ribbon(aes(x = Result, ymin = DCDF, ymax = CDF), fill = \u0026quot;red4\u0026quot;, alpha = 0.5) + geom_line(aes(x = Result, y = CDF), size = 2) + xlab(\u0026quot;d20 Target\u0026quot;) + ylab(\u0026quot;Odds of result equaling or exceeding target\u0026quot;) + theme_solarized() We see here that at the extreme ends the bonus or penalty is not very large, but at 11 the bonus or penalty is much larger, exceeding ±5. Let’s quantify the bonus or penalty and visualize it.\nADPMF$Advantage \u0026lt;- (ADPMF$ACDF - ADPMF$CDF)/0.05 ADPMF$Disadvantage \u0026lt;- (ADPMF$DCDF - ADPMF$CDF)/0.05 ADPMF %\u0026gt;% gather(`Advantage`, `Disadvantage`, key = \u0026quot;category\u0026quot;, value = \u0026quot;Bonus\u0026quot;) %\u0026gt;% select(Result, category, Bonus, ACDF, CDF, DCDF) %\u0026gt;% ggplot(aes(x = Result, y = Bonus, fill = category)) + geom_bar(stat = \u0026quot;identity\u0026quot;, dodge = \u0026quot;position\u0026quot;, alpha = 0.5) + xlab(\u0026quot;d20 Target\u0026quot;) + ylab(\u0026quot;Effective Bonus\u0026quot;) + scale_fill_manual(values = c(\u0026quot;green4\u0026quot;, \u0026quot;red4\u0026quot;)) + theme_solarized() + scale_y_continuous(breaks = -5:5, minor_breaks = -5:5) + scale_x_continuous(breaks = seq(0,20, by = 5), minor_breaks = 1:20) + ylab(\u0026quot;Effective bonus modifier\u0026quot;) ## Warning: Ignoring unknown parameters: dodge Here we can see more clearly the greatest change is when the target of the roll is 11, and it gets smaller as it becomes more extreme.\nNow, my question was not just on Advantage, but how granting Advantage compared to bonus damage on an Attack. As it turns out, this can get pretty complex based on the varied kinds of attacks there are, their damage dice etc. So let’s look at a small set of situations.\nLet’s assume we have a 1st level player. Like all level 1 characters, they have a +2 proficiency bonus, and let’s assume that their ability modifier for their weapon attack and damage is +3, the highest you could have with the Standard Array. Under this situation I will look at each of the different weapon’s damage dice. This gives a total bonus to hit of +5 (+2 proficiency bonus plus their +3 ability modifier), so we’ll look at targets with AC of 7 to 24 (AC 6 and below has the same odds of hitting as an AC 7 for someone with a +5, as well as anything higher than AC 24 will be equally hard to hit as AC 24).\nFirst, let’s calculate the average damage per attack. That means taking the odds of getting a regular hit times the average damage for a regular hit plus the odds of getting a critical hit times the average damage from a critical hit. First let’s make some tables for this for Advantage and under regular circumstances\ntoHit \u0026lt;- 5 damagePlus \u0026lt;- 3 ACRange \u0026lt;- 7:24 Target \u0026lt;- ACRange - toHit avgDieDamage \u0026lt;- c(2.5, 3.5, 4.5, 5.5, 6.5, 7) AdamageResults \u0026lt;- matrix(nrow = length(Target),ncol = length(avgDieDamage)) RdamageResults \u0026lt;- matrix(nrow = length(Target),ncol = length(avgDieDamage)) for (i in 1:length(Target)) { for (j in 1:length(avgDieDamage)) { AdamageResults[i, j] \u0026lt;- sum(ADPMF$AdvP[as.character(Target[i]:19)]) * (avgDieDamage[j] + damagePlus) + ADPMF$AdvP[\u0026#39;20\u0026#39;] * (2*avgDieDamage[j] + damagePlus) RdamageResults[i, j] \u0026lt;- sum(ADPMF$RegP[as.character(Target[i]:19)]) * (avgDieDamage[j] + damagePlus) + ADPMF$RegP[\u0026#39;20\u0026#39;] * (2*avgDieDamage[j] + damagePlus) } } Then we can calculate the difference, and visualize it:\nAboost \u0026lt;- as_tibble(AdamageResults - RdamageResults) ## Warning: The `x` argument of `as_tibble.matrix()` must have unique column names if `.name_repair` is omitted as of tibble 2.0.0. ## Using compatibility `.name_repair`. colnames(Aboost) \u0026lt;- c(\u0026quot;1d4\u0026quot;, \u0026quot;1d6\u0026quot;, \u0026quot;1d8\u0026quot;, \u0026quot;1d10\u0026quot;, \u0026quot;1d12\u0026quot;, \u0026quot;2d6\u0026quot;) Aboost$AC \u0026lt;- ACRange Aboost %\u0026gt;% gather(`1d4`, `1d6`, `1d8`, `1d10`, `1d12`, `2d6`, key = \u0026quot;damageDice\u0026quot;, value = \u0026quot;bonusDamage\u0026quot;, factor_key = T) %\u0026gt;% ggplot(aes(x = AC, y = bonusDamage, fill = damageDice)) + geom_bar(stat = \u0026quot;identity\u0026quot;, position = \u0026quot;dodge\u0026quot;) + xlab(\u0026quot;AC of Target\u0026quot;) + ylab(\u0026quot;Averge damage increase per attack\u0026quot;) + scale_fill_manual(name = \u0026quot;Damage Dice\u0026quot;, labels = c(\u0026quot;1d4\u0026quot;, \u0026quot;1d6\u0026quot;, \u0026quot;1d8\u0026quot;, \u0026quot;1d10\u0026quot;, \u0026quot;1d12\u0026quot;, \u0026quot;2d6\u0026quot;), values = c(\u0026quot;#b58900\u0026quot;, \u0026quot;#dc322f\u0026quot;, \u0026quot;#6c71c4\u0026quot;, \u0026quot;#268bd2\u0026quot;, \u0026quot;#2aa198\u0026quot;, \u0026quot;#859900\u0026quot;)) + theme_solarized() Something that I notice right away, is that for a given Damage Dice, the shape of the curve follows the bonus to succeed from Advantage.\nNow, this wasn’t exactly the question. To answer this, I need to calculate additional tables for the regular scenario, but with varying bonuses to damage. I do this from a +1 to +13 damage. I compare each of these, along with the Target AC and the Damage Dice type to find the closest amount of bonus damage to the effective increase from having Advantage.\nAdamageResults2 \u0026lt;- rep(list(matrix(nrow = length(Target),ncol = length(avgDieDamage))), 13) DdamageResults2 \u0026lt;- rep(list(matrix(nrow = length(Target),ncol = length(avgDieDamage))), 13) RdamageResults2 \u0026lt;- rep(list(matrix(nrow = length(Target),ncol = length(avgDieDamage))), 13) closestMatrix \u0026lt;- matrix(nrow = length(Target),ncol = length(avgDieDamage)) bonusMatrix \u0026lt;- matrix(nrow = length(Target),ncol = length(avgDieDamage)) for (k in 1:13) { for (i in 1:length(Target)) { for (j in 1:length(avgDieDamage)) { RdamageResults2[[k]][i, j] \u0026lt;- sum(ADPMF$RegP[as.character(Target[i]:19)]) * (avgDieDamage[j] + damagePlus + k) + ADPMF$RegP[\u0026#39;20\u0026#39;] * (2*avgDieDamage[j] + damagePlus + k) if (k \u0026gt; 1) { if (abs(AdamageResults[i, j] - RdamageResults2[[k]][i, j]) \u0026lt; abs(closestMatrix[i, j])) { closestMatrix[i, j] \u0026lt;- RdamageResults2[[k]][i, j] - AdamageResults[i, j] bonusMatrix[i, j] \u0026lt;- k } } else { closestMatrix[i, j] \u0026lt;- RdamageResults2[[1]][i, j] - AdamageResults[i, j] bonusMatrix[i, j] \u0026lt;- 1 } } } } Aboost2 \u0026lt;- as_tibble(bonusMatrix) colnames(Aboost2) \u0026lt;- c(\u0026quot;1d4\u0026quot;, \u0026quot;1d6\u0026quot;, \u0026quot;1d8\u0026quot;, \u0026quot;1d10\u0026quot;, \u0026quot;1d12\u0026quot;, \u0026quot;2d6\u0026quot;) Aboost2$AC \u0026lt;- ACRange Aboost2 %\u0026gt;% gather(`1d4`, `1d6`, `1d8`, `1d10`, `1d12`, `2d6`, key = \u0026quot;damageDice\u0026quot;, value = \u0026quot;bonusDamage\u0026quot;, factor_key = T) %\u0026gt;% ggplot(aes(x = AC, y = bonusDamage, fill = damageDice)) + geom_bar(stat = \u0026quot;identity\u0026quot;, position = \u0026quot;dodge\u0026quot;) + xlab(\u0026quot;AC of Target\u0026quot;) + ylab(\u0026quot;Damage bonus equivalent to gaining Advantage\u0026quot;) + scale_fill_manual(name = \u0026quot;Damage Dice\u0026quot;, labels = c(\u0026quot;1d4\u0026quot;, \u0026quot;1d6\u0026quot;, \u0026quot;1d8\u0026quot;, \u0026quot;1d10\u0026quot;, \u0026quot;1d12\u0026quot;, \u0026quot;2d6\u0026quot;), values = c(\u0026quot;#b58900\u0026quot;, \u0026quot;#dc322f\u0026quot;, \u0026quot;#6c71c4\u0026quot;, \u0026quot;#268bd2\u0026quot;, \u0026quot;#2aa198\u0026quot;, \u0026quot;#859900\u0026quot;)) + scale_y_continuous(breaks = seq(0,15, by = 5), minor_breaks = 0:15) + theme_solarized() Now, this initially surprised me, as it is a monotone increasing result. However, as I got to thinking about it, it made more sense to me. Initially as you go from low to mid AC targets, your bonus to hit is substantial with Advantage, so you need to keep on getting a greater damage bonus to equal the average damage per attack. However, as these numbers go higher, a greater proportion of the average damage per round with advantage comes from critical hits, which provides a significant effective damage boost.\nNow, looking at these graphs, a player or DM may ask how they can use them. The first three illustrate well how the Advantage mechanic influences results, both success and failure. When it comes to designing adventures, some DMs will focus on making sure the players feel like they are getting things done, that they succeed a large enough portion of their ability checks and attack rolls, adding to encounters in other ways to balance the game out. In this case, you can make your players feel even more epic by liberally granting Advantage when their target is around 11, as it makes it all the more epic to succeed. Similarly, failure through Disadvantage can help bring tension in a task of similar difficulty.\nAs for the final graph, this is more of a look at the internals of one of the core mechanics of Fifth edition Dungeons and Dragons, and how someone might get a similar result by going a different way. Personally, I think the Advantage/Disadvantage mechanic is an elegant solution to rewarding/punishing characters for their circumstances in a fun and easy way.\n","date":"2020-12-28","permalink":"http://blog.elementaldatascience.com/2020/12/28/the-effect-of-the-advantage-disadvantage-in-dungeons-and-dragons-5e-on-success-failure-and-damage/","tags":["R","Dungeons and Dragons","ggplot","Simulation"],"title":"The Effect of the Advantage/Disadvantage in Dungeons and Dragons 5E on Success, Failure and Damage"},{"content":"  Time for something a bit different from my previous posts, but hopefully more common in the future.\nI was looking through the UCI Machine Learning Repository for a couple of data sets I could use for some simple machine learning problems to try interesting problems and keep my abilities sharp. This week I found the Early Stage Diabetes Risk Prediction. This comes from a paper on early prediction of diabetes risk. The abstract goes over the methods used, indicating Random Forest as doing particularly well. Since I don’t have access to the actual paper, I wanted to try out using R to try this model out.\nIt turns out, that this data isn’t terribly interesting when using Random Forest. Not much cleaning up or tuning is needed to get good accuracy. For the sake of completeness though, I’ll share my snippets and results.\nFirst, we need to load the appropriate libraries and the data set. Here I load the columns except for age as factors, as well as clean up the column names.\nlibrary(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ── ## ✓ ggplot2 3.3.3 ✓ purrr 0.3.4 ## ✓ tibble 3.1.2 ✓ dplyr 1.0.6 ## ✓ tidyr 1.1.3 ✓ stringr 1.4.0 ## ✓ readr 1.4.0 ✓ forcats 0.5.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() library(randomForest) ## randomForest 4.6-14 ## Type rfNews() to see new features/changes/bug fixes. ## ## Attaching package: \u0026#39;randomForest\u0026#39; ## The following object is masked from \u0026#39;package:dplyr\u0026#39;: ## ## combine ## The following object is masked from \u0026#39;package:ggplot2\u0026#39;: ## ## margin library(reprtree) ## Loading required package: tree ## Registered S3 method overwritten by \u0026#39;tree\u0026#39;: ## method from ## print.tree cli ## Loading required package: plotrix ## Registered S3 method overwritten by \u0026#39;reprtree\u0026#39;: ## method from ## text.tree tree diabetesdf \u0026lt;- read_csv(\u0026quot;https://archive.ics.uci.edu/ml/machine-learning-databases/00529/diabetes_data_upload.csv\u0026quot;, col_types = \u0026quot;dffffffffffffffff\u0026quot;) colnames(diabetesdf) \u0026lt;- make.names(colnames(diabetesdf)) names(diabetesdf)[names(diabetesdf) == \u0026#39;class\u0026#39;] \u0026lt;- \u0026quot;result\u0026quot; Now, so we can test our out of sample error rate, let’s split 80% of the data for training our model and then test it with the remaining 20%\nsample_size = round(nrow(diabetesdf)*.80) index \u0026lt;- sample(seq_len(nrow(diabetesdf)), size = sample_size) train \u0026lt;- diabetesdf[index, ] test \u0026lt;- diabetesdf[-index, ] trainboost \u0026lt;- train testboost \u0026lt;- test trainboost$result \u0026lt;- as.numeric(trainboost$result) - 1 testboost$result \u0026lt;- as.numeric(testboost$result) - 1 Now, lets train our Random Forest model using the default parameters and calculate the accuracy.\nddffit \u0026lt;- randomForest(result ~., data = train) ddftest \u0026lt;- predict(ddffit, newdata = test, type = \u0026quot;response\u0026quot;) sum(ddftest == test$result)/nrow(test) ## [1] 0.9711538 This gets an accuracy in prediction of around 96%.\nNow, just for the sake of giving an interesting visualization, let’s use the reptree package by Abhijit Dasgupta. This enables us to create a visualization for the entirety of the Random Forest model made with the training set.\nplot(ReprTree(ddffit, train, metric = \u0026#39;d2\u0026#39;)) ## [1] \u0026quot;Constructing distance matrix...\u0026quot; ## [1] \u0026quot;Finding representative trees...\u0026quot; So, while this isn’t a particularly challenging data set for Random Forest to classify, it does provide an illustration on the basics of creating a model using Random Forest, as well as to try out reptree to make a visualization for the overall decision tree.\nSince I know this data set works well and have more experience with implementing Machine Learning algorithms in R, I will come back to this to transfer more of my skill over to Python.\n","date":"2020-12-24","permalink":"http://blog.elementaldatascience.com/2020/12/24/using-r-and-random-forest-to-predict-diabetes-risk/","tags":["R","Random Forest","Machine Learning","Classification","Data Science"],"title":"Using R and Random Forest to predict Diabetes Risk"},{"content":"  So, after sharing my post from last week about probabilities and stat arrays. Now, the way I determine which array is better is clearly overly arbitrary. Under that system the standard array is better than an array of (7, 18, 18, 18, 18, 18), when in reality any player looking for maximal stats would take that over the standard array (8, 10, 12, 13, 14, 15). So maybe there is a different way to compare these.\nNow the first approach would be to just take the ability modifier. Each integer from 3-18 maps to an integer from -4 to +4 that is used for almost everything in the game (except for carrying capacity, which uses the actual Strength score).\nNow this is great, however I want to be thorough. While in terms of in-game actions, a 10 is no different from an 11, having a higher score makes it easier to bump up the Ability Modifier later when a character gets leveled. So let’s use a modified scale. For the purpose of this, lets use a scale I found for 3.5E Dungeons and Dragons that I found while writing last week.\nSo, we’ll skip the simulation this week, and let’s start by generating our data table along with the sum of ability modifiers for each array:\nlibrary(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ── ## ✓ ggplot2 3.3.3 ✓ purrr 0.3.4 ## ✓ tibble 3.1.2 ✓ dplyr 1.0.6 ## ✓ tidyr 1.1.3 ✓ stringr 1.4.0 ## ✓ readr 1.4.0 ✓ forcats 0.5.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() statCombinations \u0026lt;- vector(mode = \u0026quot;list\u0026quot;, length = 54264) statPermutations \u0026lt;- vector(mode = \u0026quot;logical\u0026quot;, length = 54264) statOdds \u0026lt;- vector(mode = \u0026quot;logical\u0026quot;, length = 54264) sumModifier \u0026lt;- vector(mode = \u0026quot;logical\u0026quot;, length = 54264) sumModifier_human \u0026lt;- vector(mode = \u0026quot;logical\u0026quot;, length = 54264) maxPermutations \u0026lt;- factorial(6) statRoll \u0026lt;- expand.grid(rep(list(1:6), 4)) statRoll \u0026lt;- apply(statRoll,1,sort) dicepdf \u0026lt;- summary(as.factor(colSums(statRoll[2:4,])))/(6^4) rolledStats \u0026lt;- c(15, 17, 16, 15, 17, 15) rolledStats \u0026lt;- sort(rolledStats) modifierVal \u0026lt;- function(a) { modifiers \u0026lt;- as.list(floor((c(3:19) - 10)/2)) names(modifiers) \u0026lt;- c(3:19) modifier \u0026lt;- 0 for (i in a) { modifier \u0026lt;- modifier + as.numeric(modifiers[as.character(i)]) } return(modifier) } diceodds \u0026lt;- function(a) { probability \u0026lt;- 1 for (i in a) { probability \u0026lt;- probability * as.numeric(dicepdf[as.character(i)]) } return(probability) } pct_sum \u0026lt;- function(..., na.rm = FALSE) { sum(..., na.rm = na.rm) * 100 } counter \u0026lt;- 0 for (i1 in 3:18) { pos1 \u0026lt;- i1 for (i2 in i1:18) { pos2 \u0026lt;- i2 for (i3 in i2:18) { pos3 \u0026lt;- i3 for (i4 in i3:18) { pos4 \u0026lt;- i4 for (i5 in i4:18) { pos5 \u0026lt;- i5 for (i6 in i5:18) { pos6 \u0026lt;- i6 counter \u0026lt;- counter + 1 statCombinations[[counter]] \u0026lt;- c(pos1, pos2, pos3, pos4, pos5, pos6) sumModifier[counter] \u0026lt;- modifierVal(statCombinations[[counter]]) sumModifier_human[counter] \u0026lt;- modifierVal(statCombinations[[counter]] + 1) statOdds[counter] \u0026lt;- diceodds(statCombinations[[counter]]) statPermutations[counter] \u0026lt;- maxPermutations / prod(factorial(summary(as.factor(statCombinations[[counter]])))) } } } } } } finaldf \u0026lt;- as_tibble(cbind(matrix(unlist(statCombinations), ncol = 6, byrow = T), statOdds, statPermutations, sumModifier, sumModifier_human)) %\u0026gt;% mutate(probability = statOdds * statPermutations) ## Warning: The `x` argument of `as_tibble.matrix()` must have unique column names if `.name_repair` is omitted as of tibble 2.0.0. ## Using compatibility `.name_repair`. So, taking our hypothesized array of (15, 17, 16, 15, 17, 15), let’s compare the odds we got last week with this new method. First, the old method:\nfinaldf %\u0026gt;% filter(V1 \u0026gt;= rolledStats[1] \u0026amp; V2 \u0026gt;= rolledStats[2] \u0026amp; V3 \u0026gt;= rolledStats[3] \u0026amp; V4 \u0026gt;= rolledStats[4] \u0026amp; V5 \u0026gt;= rolledStats[5] \u0026amp; V6 \u0026gt;= rolledStats[6]) %\u0026gt;% summarize_at(vars(probability), list(total_percent = pct_sum)) ## # A tibble: 1 x 1 ## total_percent ## \u0026lt;dbl\u0026gt; ## 1 0.00665 This is 0.0066%, like we had last week. Now lets do the new way when comparing the sum total of ability modifiers:\nfinaldf %\u0026gt;% filter(sumModifier \u0026gt;= modifierVal(rolledStats)) %\u0026gt;% summarize_at(vars(probability), list(total_percent = pct_sum)) ## # A tibble: 1 x 1 ## total_percent ## \u0026lt;dbl\u0026gt; ## 1 0.268 This time we get 0.2684%. This is a lot higher than the previous way, over 40 times in fact. However, even with that, it’s still less than a 1% chance, so I still would say it’s likely to be a cheating player (though that conclusion is not so strong as the old method).\nOne thing this new method allows, is a lot mor intuition on what arrays are “better” than others, as well as to plot the distribution of these stat arrays. Lets plot this distribution.\nmodifierdist \u0026lt;- finaldf %\u0026gt;% group_by(sumModifier) %\u0026gt;% summarize_at(vars(probability), list(total_percent = pct_sum)) modifierdist %\u0026gt;% ggplot(aes(x = sumModifier, y = total_percent)) + geom_bar(stat = \u0026quot;identity\u0026quot;) + ggtitle(\u0026quot;Distribution of Stat Blocks Before Racial Modifiers\u0026quot;) + ylab(\u0026quot;% frequency\u0026quot;) + xlab(\u0026quot;Sum of Ability Modifiers\u0026quot;) The next thing we can do is we can actually calculate summary statistics using the definitions of median, mode, mean and standard deviation for discrete distributions like this. So let’s do it.\n#median modifierdist$sumModifier[50 \u0026lt;= cumsum(modifierdist$total_percent)][1] ## [1] 5 #mode modifierdist$sumModifier[which.max(modifierdist$total_percent)] ## [1] 5 #mean sum(modifierdist$sumModifier * modifierdist$total_percent/100) ## [1] 5.240741 #standard deviation sqrt(sum((modifierdist$sumModifier)^2 * modifierdist$total_percent/100) - (sum(modifierdist$sumModifier * modifierdist$total_percent/100))^2) ## [1] 3.544196 This gives a median and mode of +5, a mean of +5.2407, and a standard deviation of 3.5442.\nNow, these stats are only at the beginning of character creation. Based on the options chosen, at least two of your ability scores will increase, but doing this for all combinations is not so much practical. However, the most vanilla option is the Human option, which gives a +1 to each of your Ability Scores. So let’s take a look at that, plot the distribution and recalculate the summary statistics.\nfinaldf_human \u0026lt;- finaldf %\u0026gt;% mutate(V1 = V1 + 1, V2 = V2 + 1, V3 = V3 + 1, V4 = V4 + 1, V5 = V5 + 1, V6 = V6 + 1) modifierdist_human \u0026lt;- finaldf_human %\u0026gt;% group_by(sumModifier_human) %\u0026gt;% summarize_at(vars(probability), list(total_percent = pct_sum)) modifierdist_human %\u0026gt;% ggplot(aes(x = sumModifier_human, y = total_percent)) + geom_bar(stat = \u0026quot;identity\u0026quot;) + ggtitle(\u0026quot;Distribution of Stat Blocks of Human Characters\u0026quot;) + ylab(\u0026quot;% frequency\u0026quot;) + xlab(\u0026quot;Sum of Ability Modifiers\u0026quot;) #median modifierdist_human$sumModifier_human[50 \u0026lt;= cumsum(modifierdist_human$total_percent)][1] ## [1] 8 #mode modifierdist_human$sumModifier_human[which.max(modifierdist_human$total_percent)] ## [1] 8 #mean sum(modifierdist_human$sumModifier_human * modifierdist_human$total_percent/100) ## [1] 8.226852 #standard deviation sqrt(sum((modifierdist_human$sumModifier_human)^2 * modifierdist_human$total_percent/100) - (sum(modifierdist_human$sumModifier_human * modifierdist_human$total_percent/100))^2) ## [1] 3.535849 The median and mode are increased by 3 to +8. The mean also goes up around 3 to +8.2269, with the standard deviation being about the same at 3.5358\nFor completeness, let’s make one final graph with the base stats and the human option next to each other.\ntmp1 \u0026lt;- finaldf %\u0026gt;% group_by(sumModifier_human) %\u0026gt;% summarize_at(vars(probability), list(total_percent = pct_sum)) tmp2 \u0026lt;- finaldf %\u0026gt;% group_by(sumModifier) %\u0026gt;% summarize_at(vars(probability), list(total_percent = pct_sum)) tmp3 \u0026lt;- rbind(tibble(sumModifier_human = rep(0,6), total_percent = rep(0,6)), tmp1) comparedf \u0026lt;- tibble(sumModifier = tmp2$sumModifier, basePercent = tmp2$total_percent, humanPercent = tmp3$total_percent) %\u0026gt;% gather(`basePercent`, `humanPercent`, key = \u0026quot;type\u0026quot;, value = \u0026quot;percent\u0026quot;) comparedf %\u0026gt;% ggplot() + geom_bar(aes(x = sumModifier, y = percent, fill = type), alpha = 0.9, width = 1, stat = \u0026quot;identity\u0026quot;, position = position_dodge(width = 1)) + ggtitle(\u0026quot;Distribution of Stat Blocks Before Racial Modifiers vs. Base Human\u0026quot;) + ylab(\u0026quot;% frequency\u0026quot;) + xlab(\u0026quot;Sum of Ability Modifiers\u0026quot;) + xlim(-10, 22) + scale_fill_manual(name = \u0026quot;Scenario\u0026quot;,labels = c(\u0026quot;Base Stats\u0026quot;, \u0026quot;Human Modifiers\u0026quot;), values = c(\u0026quot;dodgerblue4\u0026quot;, \u0026quot;firebrick4\u0026quot;)) ## Warning: Removed 34 rows containing missing values (geom_bar). We see that there’s about a +3 shift to the right, though if you look carefully it isn’t a simple shift of 3. For a simple example, the minimum value of the sum for base stats is -24, where for the Human option shifts that all the way to -18.\n","date":"2020-12-21","permalink":"http://blog.elementaldatascience.com/2020/12/21/the-likelihood-of-getting-or-beating-an-array-of-stats-in-d-d-part-2/","tags":["R","Probability Distribution","Dungeons and Dragons","Simulation"],"title":"The likelihood of getting or beating an array of stats in D\u0026D Part 2"},{"content":"  I got involved in another conversation about Dungeons and Dragons and probabilities again. This time I saw something that I’ve seen come up in a variety of circumstances. So, let’s take this scenario.\nA Dungeon Master is running a game where the players roll for their stats, and one player has suspiciously high results. The question of course is, did the player cheat with their stats, either by rolling a different way, or did they just pick the stats they wanted. Given D\u0026amp;D is a cooperative game, this can be especially frustrating to the other players at the table, as this one player has a character that is well above average for doing just about everything.\nFirst, lets talk about the standard way for generating stats for a character. In D\u0026amp;D 5th edition, the default way is actually just to take a standard array of values. 8, 10, 12, 13, 14, 15. The nice thing about this is that it gets rid of inequality from good or bad luck of a player. The downside for many, is that it’s just too predictable. They like the chance of having a character with some really high stats and low stats as well. The standard way of rolling for stats is called 4d6, drop lowest. What that means is for each of the six stats, you roll 4 six-sided dice, discard the single lowest die, and then sum the remaining 3 dice. This gives a range of numbers from 3 to 18 that is skewed in a way to make the players have results that are a bit higher than average in that range compared to just rolling 3 dice.\nWith all my previous posts I’ve done simulations, so lets start off with that. Lets take an unbelievably high set of stats: 15, 17, 16, 15, 17, 15 and then simulate creating a million characters and count what proportion meet or exceed this array of stats. In R this is pretty straight forward:\nlibrary(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ── ## ✓ ggplot2 3.3.3 ✓ purrr 0.3.4 ## ✓ tibble 3.1.2 ✓ dplyr 1.0.6 ## ✓ tidyr 1.1.3 ✓ stringr 1.4.0 ## ✓ readr 1.4.0 ✓ forcats 0.5.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() rolledStats \u0026lt;- c(15, 17, 16, 15, 17, 15) rolledStats \u0026lt;- sort(rolledStats) die_sides \u0026lt;- 1:6 N \u0026lt;- 1000000 results \u0026lt;- vector(mode = \u0026quot;list\u0026quot;, length = N) count \u0026lt;- 0 for (j in 1:N) { results[[j]] \u0026lt;- vector(mode = \u0026quot;logical\u0026quot;, length = 6) for (k in 1:6) { results[[j]][k] \u0026lt;- sum(sort(sample(die_sides, 4, replace = T), decreasing = TRUE)[1:3]) } results[[j]] \u0026lt;- sort(results[[j]]) } countExceeding \u0026lt;- as_tibble(matrix(unlist(results), ncol = 6, byrow = T)) %\u0026gt;% filter(V1 \u0026gt;= rolledStats[1] \u0026amp; V2 \u0026gt;= rolledStats[2] \u0026amp; V3 \u0026gt;= rolledStats[3] \u0026amp; V4 \u0026gt;= rolledStats[4] \u0026amp; V5 \u0026gt;= rolledStats[5] \u0026amp; V6 \u0026gt;= rolledStats[6]) %\u0026gt;% count() ## Warning: The `x` argument of `as_tibble.matrix()` must have unique column names if `.name_repair` is omitted as of tibble 2.0.0. ## Using compatibility `.name_repair`. paste(countExceeding[[1]]/N*100, \u0026quot;%\u0026quot;, sep = \u0026quot;\u0026quot;) ## [1] \u0026quot;0.0067%\u0026quot; When running this I got 0.0097%, which I must say, is pretty unlikely.\nBut what if we could do better and calculate this from first principles. For each roll of 4d6, drop lowest there are only \\(6^4 = 1296\\) outcomes, so counting the probability of each value from 3 to 18 should be easy enough. We can use these probabilities to calculate the odds of each 54,264 unique arrays of stats we can get by calculating the odds of one of the permutations of those arrays as well as the number of permutations of that arrays. We can then filter through the resulting table to find all the arrays that are as good or better than the array we are testing against. It may sound like a lot of computation, but by being smart about the approach I get it run in under 5 seconds from start to finish.\ncounter \u0026lt;- 0 statCombinations \u0026lt;- vector(mode = \u0026quot;list\u0026quot;, length = 54264) statPermutations \u0026lt;- vector(mode = \u0026quot;logical\u0026quot;, length = 54264) statOdds \u0026lt;- vector(mode = \u0026quot;logical\u0026quot;, length = 54264) maxPermutations \u0026lt;- factorial(6) diceodds \u0026lt;- function(a) { statRoll \u0026lt;- expand.grid(rep(list(1:6), 4)) statRoll \u0026lt;- apply(statRoll,1,sort) dicepdf \u0026lt;- summary(as.factor(colSums(statRoll[2:4,])))/(6^4) probability \u0026lt;- 1 for (i in a) { probability \u0026lt;- probability * as.numeric(dicepdf[as.character(i)]) } return(probability) } pct_sum \u0026lt;- function(..., na.rm = FALSE) { sum(..., na.rm = na.rm) * 100 } for (i1 in 3:18) { pos1 \u0026lt;- i1 for (i2 in i1:18) { pos2 \u0026lt;- i2 for (i3 in i2:18) { pos3 \u0026lt;- i3 for (i4 in i3:18) { pos4 \u0026lt;- i4 for (i5 in i4:18) { pos5 \u0026lt;- i5 for (i6 in i5:18) { pos6 \u0026lt;- i6 counter \u0026lt;- counter + 1 statCombinations[[counter]] \u0026lt;- c(pos1, pos2, pos3, pos4, pos5, pos6) statOdds[counter] \u0026lt;- diceodds(statCombinations[[counter]]) statPermutations[counter] \u0026lt;- maxPermutations / prod(factorial(summary(as.factor(statCombinations[[counter]])))) } } } } } } prob \u0026lt;- as_tibble(cbind(matrix(unlist(statCombinations), ncol = 6, byrow = T), statOdds, statPermutations)) %\u0026gt;% mutate(probability = statOdds * statPermutations) %\u0026gt;% filter(V1 \u0026gt;= rolledStats[1] \u0026amp; V2 \u0026gt;= rolledStats[2] \u0026amp; V3 \u0026gt;= rolledStats[3] \u0026amp; V4 \u0026gt;= rolledStats[4] \u0026amp; V5 \u0026gt;= rolledStats[5] \u0026amp; V6 \u0026gt;= rolledStats[6]) %\u0026gt;% summarize_at(vars(probability), list(total_percent = pct_sum)) paste(prob[[1]], \u0026quot;%\u0026quot;, sep = \u0026quot;\u0026quot;) ## [1] \u0026quot;0.00664599433876074%\u0026quot; Calculating from first principles gives us a probability of 0.0066%, a little bit lower than the result of my simulation, but in neighborhood.\nI made a Shiny App so you can put in any given array in to see it’s probability. The default is the standard array, and when the odds of beating each stat in the standard array come out to be around 32%, it doesn’t look so bad.\nOf course, if you run across this situation in a game, there is still that uncertainty of did they really cheat. Given the social nature of D\u0026amp;D you could offend the player by accusing them of something they didn’t actually do. Or, even though they cheated, it might have to do with their own insecurities in wanting to not fail or wanting to be an epic hero. No matter the case, what tends to be the most well received advice in the conversations I’ve seen about this is to without mentioning cheating, talk to the player about the effect on the enjoyment of their fellow players along with the suggestion of them roll their stats again.\n","date":"2020-12-15","permalink":"http://blog.elementaldatascience.com/2020/12/15/the-likelihood-of-getting-or-beating-an-array-of-stats-in-d-d/","tags":[],"title":"The likelihood of getting or beating an array of stats in D\u0026D"},{"content":"  Back in August of this year, the game Among Us leapt in popularity. A social game where crewmates try to complete tasks while staying alive. At the same time, impostors try to kill off the innocent players while also sabotaging the crew. It’s a fun, interactive twist on in person social games such as Werewolf or Mafia.\nWith the interactive gameplay, rounds end when a player reports a dead body or call an emergency meeting and the remaining players get a chance to discus and potentially vote out one player.\nThis got me thinking, ignoring the various strategies for the game, and avoiding where impostors kill off innocent players, if players were just removed/voted off randomly, what would be the odds of a crewmate or an impostor victory, and with that, the expected win rate overall.\nCalculating the probabilities under these assumptions isn’t too difficult, but designing a simulation to do this is also very straightforward. First, let’s simulate a single Among Us game.\nIt’s important to understand the victory criteria for the crewmates and impostors in this setup. The crewmates win this way when all the impostors have been voted off before the number of crewmates gets reduced to the same number of remaining impostors. The standard game is with 10 players, with two of them being impostors. We can write a function in R that takes in variables for the number of players and number of impostors and then returns a 1 if the crewmates win, and a 0 if the impostors win.\nlibrary(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ── ## ✓ ggplot2 3.3.3 ✓ purrr 0.3.4 ## ✓ tibble 3.1.2 ✓ dplyr 1.0.6 ## ✓ tidyr 1.1.3 ✓ stringr 1.4.0 ## ✓ readr 1.4.0 ✓ forcats 0.5.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() amongusGame \u0026lt;- function(players = 10, impostors = 2) { # return 1 on crewmate victory crewmates \u0026lt;- players - impostors while (crewmates != impostors \u0026amp; impostors != 0) { voted \u0026lt;- sample(c(\u0026quot;impostor\u0026quot;, \u0026quot;crewmate\u0026quot;), 1, prob = c(impostors, crewmates)) if (voted == \u0026quot;impostor\u0026quot;) { impostors \u0026lt;- impostors - 1 } else { crewmates \u0026lt;- crewmates - 1 } } if (impostors == 0) { return(1) } else { return(0) } } Now that we have a function that simulates a single game, we can simply write a wrapper function to repeat it an arbitrary number of times, as well as compute the odds of crewmates winning this way.\namongusSim \u0026lt;- function(players = 10, impostors = 2, N = 2000){ results \u0026lt;- vector(mode = \u0026quot;logical\u0026quot;, length = N) for (i in 1:N) { results[i] \u0026lt;- amongusGame(players, impostors) } return(results) } Now, we can have a bit of fun by visualizing what the perceived win rate is based on how many games it has been since we started to count. We can make a basic line graph for this.\namongusPlot \u0026lt;- function(players = 10, impostors = 2, N = 2000){ winresults \u0026lt;- amongusSim(players, impostors, N) print(sum(winresults)/N) winrate \u0026lt;- vector(mode = \u0026quot;logical\u0026quot;, length = N) for (i in 1:length(winresults)) { winrate[i] \u0026lt;- sum(winresults[1:i])/i } ggplot(tibble(gamenum = 1:N, winrate = winrate), aes(x = gamenum, y = winrate )) + geom_line() + xlab(\u0026quot;Game #\u0026quot;) + ylab(\u0026quot;Win Rate\u0026quot;) + ylim(0,1) } amongusPlot() ## [1] 0.5975 We can see that from about 1000 onward, the result seems to arrive at 0.6\nSo apparently randomly voting for a player gives the crewmates a slight advantage. From this we can then calculate what the expected win rate of each player if every game was played this way.\n\\((0.6)*0.8 + (0.4)*0.2 = 0.56\\)\nWhich means just by guessing randomly, including players voting for themselves, everyone would win about 56% of their games.\nOf course, this is just an exercise in odds, given the game is much more complex than these assumptions (like the impostors would never vote for themselves or their fellow impostor), especially when looking at the various strategies that can be employed.\n","date":"2020-12-07","permalink":"http://blog.elementaldatascience.com/2020/12/07/probabilities-of-winning-in-the-game-among-us/","tags":[],"title":"Probabilities of Winning in the game Among Us"},{"content":"  As someone who enjoys playing Dungeons and Dragons, I also get fascinated by the primitive random number generators used it them, the dice. When I talk dice with friends and internet strangers, the conversation sometimes leans towards discussing whether or not their dice are fair.\nNow if you search for how to test this, you can find a few good answers to explain the various statistical tests that can be used, along with their pros and their cons, so I’m not going to do that here.\nWhile probabilities of several dice together can get messy really quickly without having to pull out a reference for my courses in Probability, one quick and dirty way we have of checking the odds of results is using simulations, and doing large simulations of die rolls is pretty quick on modern machines.\nSo, for this first simulation I wanted to take a d20, a 20-sided die and simulate the results of a chi-squared test with a decent sample size, enough to expect each result 200 times, plot a histogram of the results, and plot a cumulative bar chart showing the odds of getting at least a certain result. Then we can attempt to quantify the fairness by doing a chi-squared test.\nlibrary(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ── ## ✓ ggplot2 3.3.3 ✓ purrr 0.3.4 ## ✓ tibble 3.1.2 ✓ dplyr 1.0.6 ## ✓ tidyr 1.1.3 ✓ stringr 1.4.0 ## ✓ readr 1.4.0 ✓ forcats 0.5.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() theme_set(theme_bw()) set.seed(1) n \u0026lt;- 4000 d \u0026lt;- 20 dfair \u0026lt;- tibble(result = sample(x = seq.int(from = 1, to = d), size = n, replace = TRUE)) dcountsfair \u0026lt;- dfair %\u0026gt;% group_by(result) %\u0026gt;% count() dcountsfair$cum \u0026lt;- rev(cumsum(rev(dcountsfair$n))) dfair %\u0026gt;% ggplot(aes(x = result)) + geom_histogram(binwidth = 1, col = \u0026quot;black\u0026quot;) + geom_hline(yintercept=n/d, linetype=\u0026quot;dashed\u0026quot;, color = \u0026quot;red\u0026quot;) dcountsfair %\u0026gt;% ggplot(aes(x = result)) + geom_bar(aes(y = cum), stat = \u0026quot;identity\u0026quot;, col = \u0026quot;black\u0026quot;) + xlab(\u0026quot;result\u0026quot;) + ylab(\u0026quot;Count equals or exceeds result\u0026quot;) chisq.test(table(dfair), p = rep(1/d, d)) ## ## Chi-squared test for given probabilities ## ## data: table(dfair) ## X-squared = 14.2, df = 19, p-value = 0.7719 Now, let’s do the same thing with a simulated die that the highest result is 10% more likely and the lowest result is 10% less likely.\nset.seed(1) dweighted \u0026lt;- tibble(result = sample(x = seq.int(from = 1, to = d), size = n, replace = TRUE, prob = c(0.9, rep(1, d - 2), 1.1))) dcountsweighted \u0026lt;- dweighted %\u0026gt;% group_by(result) %\u0026gt;% count() dcountsweighted$cum \u0026lt;- rev(cumsum(rev(dcountsweighted$n))) dweighted %\u0026gt;% ggplot(aes(x = result)) + geom_histogram(binwidth = 1, col = \u0026quot;black\u0026quot;) + geom_hline(yintercept=n/d, linetype=\u0026quot;dashed\u0026quot;, color = \u0026quot;red\u0026quot;) dcountsweighted %\u0026gt;% ggplot(aes(x = result)) + geom_bar(aes(y = cum), stat = \u0026quot;identity\u0026quot;, col = \u0026quot;black\u0026quot;) + xlab(\u0026quot;result\u0026quot;) + ylab(\u0026quot;Count equals or exceeds result\u0026quot;) chisq.test(table(dweighted), p = rep(1/d, d)) ## ## Chi-squared test for given probabilities ## ## data: table(dweighted) ## X-squared = 32.12, df = 19, p-value = 0.0303 To the casual observer, the histogram and cumulative bar plots don’t really look that different from each other. Sure, the number of 20’s are higher in the weighted simulation, but it was also higher in the fair simulation. But here we see the results of the Chi-squared test reveal things are quite a bit different in these results.\nOf course detecting this deviation is a function of how severe the problem is versus the sample size.\nNext time I’ll be looking at a different test with the same simulation, the Kolmogorov-Smirnov test.\n","date":"2020-11-24","permalink":"http://blog.elementaldatascience.com/2020/11/24/simulating-die-rolls-and-testing-fairness-using-a-chi-squared-test./","tags":["R Markdown","plot","regression"],"title":"Simulating die rolls and testing fairness using a Chi-squared test."}]