<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R on Data Communications</title>
    <link>/tags/r/</link>
    <description>Recent content in R on Data Communications</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 26 May 2021 00:00:00 +0000</lastBuildDate><atom:link href="/tags/r/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Multivariate Statistic Process Control method comparison Part 2: Generating   control limits and initial look </title>
      <link>/2021/05/26/multivariate-statistic-process-control-method-comparison-part-2-generating-control-limits-and-initial-look/</link>
      <pubDate>Wed, 26 May 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/05/26/multivariate-statistic-process-control-method-comparison-part-2-generating-control-limits-and-initial-look/</guid>
      <description>Earlier this year, I posted the first part of a write-up for a project I worked on during a project I worked on last year.
As a reminder, my project involved taking 4 different types of correlation, test the baseline and 8 different data shifts, generating 1000 simulations with 100 points of data each. I then compared how three different multivariate control charts, Hotelling T^2, MEWMA, and MCUSUM, were in detecting a shift while holding the false rate fixed over all simulations.</description>
    </item>
    
    <item>
      <title>Multivariate Statistic Process Control method comparison Part 1: Simulating   correlated data and optimizing data generation.</title>
      <link>/2021/01/04/multivariate-statistic-process-control-method-comparison-part-1-simulating-correlated-data-and-optimizing-data-generation/</link>
      <pubDate>Mon, 04 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/01/04/multivariate-statistic-process-control-method-comparison-part-1-simulating-correlated-data-and-optimizing-data-generation/</guid>
      <description>Time to mix it up a bit from my usual game probability related simulation. This week I want to talk about a project I worked on last year in doing simulations for Statistical Process Control.
For a brief primer, Statistical Process Control or SPC is a method used for monitoring something like a manufacturing process. It&amp;rsquo;s a form of time series analysis where you monitor results of some statistic taken from observing the process and then compare that statistic to a set of limits on a control chart to determine whether or not the distribution of that statistic has likely changed.</description>
    </item>
    
    <item>
      <title>The Effect of the Advantage/Disadvantage in Dungeons and Dragons 5E on Success, Failure and Damage</title>
      <link>/2020/12/28/the-effect-of-the-advantage-disadvantage-in-dungeons-and-dragons-5e-on-success-failure-and-damage/</link>
      <pubDate>Mon, 28 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/12/28/the-effect-of-the-advantage-disadvantage-in-dungeons-and-dragons-5e-on-success-failure-and-damage/</guid>
      <description>So, after my last Dungeons and Dragons post, I got asked about the Advantage/Disadvantage mechanic in Dungeons and Dragons Fifth Edition, so let’s look into that a bit.
In D&amp;amp;D, to succeed at a task, you roll a 20-sided die(d20) and with some math and checking with your Dungeon Master (DM), you determine if you succeed or not. In previous editions you would add your character’s modifier, as well as any additional bonuses or penalties due to the specific scenario.</description>
    </item>
    
    <item>
      <title>Using R and Random Forest to predict Diabetes Risk</title>
      <link>/2020/12/24/using-r-and-random-forest-to-predict-diabetes-risk/</link>
      <pubDate>Thu, 24 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/12/24/using-r-and-random-forest-to-predict-diabetes-risk/</guid>
      <description>Time for something a bit different from my previous posts, but hopefully more common in the future.
I was looking through the UCI Machine Learning Repository for a couple of data sets I could use for some simple machine learning problems to try interesting problems and keep my abilities sharp. This week I found the Early Stage Diabetes Risk Prediction. This comes from a paper on early prediction of diabetes risk.</description>
    </item>
    
    <item>
      <title>The likelihood of getting or beating an array of stats in D&amp;D Part 2</title>
      <link>/2020/12/21/the-likelihood-of-getting-or-beating-an-array-of-stats-in-d-d-part-2/</link>
      <pubDate>Mon, 21 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/12/21/the-likelihood-of-getting-or-beating-an-array-of-stats-in-d-d-part-2/</guid>
      <description>So, after sharing my post from last week about probabilities and stat arrays. Now, the way I determine which array is better is clearly overly arbitrary. Under that system the standard array is better than an array of (7, 18, 18, 18, 18, 18), when in reality any player looking for maximal stats would take that over the standard array (8, 10, 12, 13, 14, 15). So maybe there is a different way to compare these.</description>
    </item>
    
  </channel>
</rss>
